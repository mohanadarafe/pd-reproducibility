{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Pipeline\n",
    "In this notebook, we will develop the machine learning models by preprocessing and training the data in four models as per Lydia Chougar's pipeline.\n",
    "\n",
    "[1. Convert CSV to DataFrame](#data)\n",
    "\n",
    "[2. Normalize data](#normalize)\n",
    "\n",
    "[3. Define models](#models)\n",
    "\n",
    "[4. Training models](#training)\n",
    "\n",
    "[5. Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, sys, os, json, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(csvFileName: str, ROI: []):\n",
    "    '''\n",
    "    The following function will sanitize data and build a numpy array with X ROI's volumes and y being the class [NC, PD]\n",
    "    @csvFileName: input volumes csv\n",
    "    @ROI: regions of interests desired\n",
    "    '''\n",
    "    df = pd.read_csv(csvFileName)\n",
    "    df = utils.remove_unwanted_columns(df, ROI)\n",
    "    df = utils.combine_left_right_vol(df)\n",
    "        \n",
    "    cols = list(df.columns.values)\n",
    "    cols.pop(cols.index(\"subjectId\"))\n",
    "    df = df[[\"subjectId\"]+cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROI = [\n",
    "      \"subjectId\",\n",
    "      \"Left-Putamen\", \"Right-Putamen\", \n",
    "      \"Right-Caudate\", \"Left-Caudate\", \n",
    "      \"Right-Thalamus-Proper\", \"Left-Thalamus-Proper\", \n",
    "      \"Left-Pallidum\", \"Right-Pallidum\", \n",
    "      \"Left-Cerebellum-White-Matter\", \"Right-Cerebellum-White-Matter\", \n",
    "      \"Left-Cerebellum-Cortex\", \"Right-Cerebellum-Cortex\",\n",
    "      \"3rd-Ventricle\", \n",
    "      \"4th-Ventricle\",\n",
    "      \"Pons\",\n",
    "      \"SCP\",\n",
    "      \"Midbrain\",\n",
    "      \"Insula\",\n",
    "      \"Precenral Cortex\"\n",
    "]\n",
    "df = get_data(\"../data/volume-data/freeSurferVolumes.csv\", ROI)\n",
    "df.to_csv(\"../data/volume-data/sanitizedVolumes.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "\n",
    "In this section, normalization of the data using \"Normalization 1\" and \"Normaliztion 2\" techniques are implemented. \n",
    "\n",
    "Normalization 1:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}{\\sigma \\;of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}$$\n",
    "\n",
    "Normalization 2:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}{\\sigma \\;of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize1(df, mean, std):\n",
    "    if mean is None and std is None:\n",
    "        mean = df.mean(axis=0)\n",
    "        std = df.std(axis=0)\n",
    "        normalizedDf = (df - mean)/std\n",
    "        return normalizedDf.values, mean, std\n",
    "\n",
    "    normalizedDf = (df - mean)/std\n",
    "    return normalizedDf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(df):\n",
    "    df_no_id = df.drop(columns=[\"subjectId\", \"stage\"])\n",
    "    metadata_df = utils.parse_metadata()\n",
    "    merged_df = pd.merge(df, metadata_df, on=[\"subjectId\"], how=\"left\")\n",
    "   \n",
    "    stats = {}\n",
    "    for scanner in merged_df[\"scannerType\"].dropna().unique():\n",
    "        mean, std = utils.get_mean_and_stats(merged_df.drop(columns=\"subjectId\"), scanner, df_no_id.shape[1])\n",
    "        stats[scanner] = {\n",
    "            \"mean\": mean.to_dict(),\n",
    "            \"std\": std.to_dict()\n",
    "        }\n",
    "\n",
    "    for index in merged_df.index:\n",
    "        rowInfo = merged_df.iloc[index]\n",
    "        scanner = rowInfo[\"scannerType\"]\n",
    "        mean = list(stats[scanner][\"mean\"].values())\n",
    "        std = list(stats[scanner][\"std\"].values())\n",
    "        df_no_id.iloc[index] = (df_no_id.iloc[index]-mean)/std\n",
    "        \n",
    "    return df_no_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "In this section, we define four models being logisitc regression, SVM with linear and radial kernel and a random forest. As per the paper:\n",
    "\n",
    "_Using the scikit-learn package, four supervised\n",
    "machine learning algorithms were used: logistic regression, support vector machine (SVM) with a linear kernel, SVM with a radial basis function kernel, and\n",
    "random forest_ (Chougar et al.)\n",
    "\n",
    "Additionally, we will implement a stratified cross validation loop for hyperparameter tuning. As per the paper:\n",
    "\n",
    "_The cross-validation procedure on the training cohort included two nested loops: an outer loop with repeated stratified random splits with 50 repetitions evaluating the classification performances and an inner loop with 5 fold cross-validation used to optimize the hyperparameters of the algorithms_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Parallel job\n",
    "from joblib import Parallel, delayed\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel model\n",
    "\n",
    "Since our cross validation loop produces 250 folds per model, it is bound to take a long time to run. Therefore, a refined version of the code above is re-written in parallel. \n",
    "\n",
    "It is recommended that you run the following code from your terminal:\n",
    "```\n",
    "conda activate research # Check README to get corect CONDA environemnt\n",
    "cd ml/\n",
    "python run.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf, train_index, test_index, X, y, normalize, columns, modelType, reportKey, iteration):\n",
    "    print(f\"=================Iteration #{iteration}=================\")\n",
    "    performanceDict = {}\n",
    "        \n",
    "    # Get fold data train/test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(f'Shape of train set: {X_train.shape}')\n",
    "    print(f'Shape of test set: {X_test.shape}')\n",
    "    \n",
    "    # Normalize model data\n",
    "    print(\"Normalizing data...\")\n",
    "    if normalize.__name__ == \"normalize1\":\n",
    "        trainDf = pd.DataFrame(X_train, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "        testDf = pd.DataFrame(X_test, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "        X_train_normalized, mean_train, std_train = normalize(trainDf, None, None)\n",
    "        X_test_normalized = normalize(testDf, mean_train, std_train)\n",
    "\n",
    "    elif normalize.__name__ == \"normalize2\":\n",
    "        trainDf = pd.DataFrame(X_train, columns=columns)\n",
    "        testDf = pd.DataFrame(X_test, columns=columns)\n",
    "        X_train_normalized = normalize2(trainDf)\n",
    "        X_test_normalized = normalize2(testDf)\n",
    "        \n",
    "    print(\"Done normalizing data\")\n",
    "        \n",
    "    print(f\"Fitting {modelType} model #{iteration}...\")\n",
    "    model = clf.fit(X_train_normalized, y_train)\n",
    "    print(\"Done fitting model\")\n",
    "    \n",
    "    print(f\"Computing results metrics for {modelType} model #{iteration}...\")\n",
    "    performanceDict = utils.performance_report(model, modelType, reportKey, iteration, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "    print(\"Done computing results metrics\\n\")\n",
    "\n",
    "    return performanceDict\n",
    "\n",
    "def parallel_model(df, modelType, reportKey, normalize, paramGrid, dataFile, ROI, heuristic=None):\n",
    "    print(f\"\\n======================Running {modelType} with the following parameters======================\\nNormalization: {normalize.__name__}\\nParam Grid: {paramGrid}\\nData: {dataFile}\\nROI: {ROI}\")\n",
    "\n",
    "    performance = []\n",
    "    if not os.path.isdir(modelType):\n",
    "        os.mkdir(modelType)\n",
    "\n",
    "    X = df.values\n",
    "    y = utils.convert_Y(X[:, -1])\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=42)\n",
    "\n",
    "    # Define model type\n",
    "    if modelType == \"SVM\":\n",
    "        clf = GridSearchCV(SVC(random_state=0), paramGrid)\n",
    "    elif modelType == \"RF\":\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=0, n_jobs = -1), paramGrid)\n",
    "    elif modelType == \"LR\":\n",
    "        clf = GridSearchCV(LogisticRegression(random_state=0), paramGrid)\n",
    "    \n",
    "    output = Parallel(n_jobs=-1)(delayed(train)(clf, train_index, test_index, X, y, normalize, columns, modelType, reportKey, iteration) for iteration, (train_index, test_index) in enumerate(cv.split(X, y)))\n",
    "\n",
    "    performance.append(output)\n",
    "\n",
    "    with open(f\"{modelType}/{reportKey}_report.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(performance, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return performance"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03eb3d269ffa8480b6c622a58387450f74b2b472a591e6bfb46288b74805594b"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
