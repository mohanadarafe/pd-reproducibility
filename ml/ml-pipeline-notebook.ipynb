{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Machine Learning pipeline\n",
    "\n",
    "In this notebook, we go through the machine learning pipeline to reproduce Lydia Chougar's paper. The following sections will be covered:\n",
    "\n",
    "1 - Convert CSV to DataFrame\n",
    "\n",
    "2 - Normalize\n",
    "\n",
    "3 - Train and predict models\n",
    "\n",
    "4 - Cross Validation\n",
    "\n",
    "5 - Results "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, utils, sys"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Convert CSV to DataFrame"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Converts data from CSV to DataFrame and applies any function. \n",
    "- \"combine\": sums all Left and Right regions into one column"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def get_data(csvFileName: str, ROI: [], heuristic = None):\n",
    "    '''\n",
    "    The following function will sanitize data and build a numpy array with X ROI's volumes and y being the class [NC, PD]\n",
    "    @csvFileName: input volumes csv\n",
    "    @ROI: regions of interests desired\n",
    "    @heuristic: function key\n",
    "    '''\n",
    "    df = pd.read_csv(csvFileName)\n",
    "    df = utils.remove_unwanted_columns(df, ROI)\n",
    "    \n",
    "    if heuristic == \"combine\":\n",
    "        df = utils.combine_left_right_vol(df)\n",
    "        \n",
    "    arr = df.values\n",
    "    X = arr[:, :-1]\n",
    "    y = utils.convert_Y(arr[:, -1])\n",
    "    return X,y"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Test *get_data()* function"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "ROI = [\n",
    "      \"class\",\n",
    "      \"Left-Putamen\", \"Right-Putamen\", \n",
    "      \"Right-Caudate\", \"Left-Caudate\", \n",
    "      \"Right-Thalamus-Proper\", \"Left-Thalamus-Proper\", \n",
    "      \"Left-Pallidum\", \"Right-Pallidum\", \n",
    "      \"Left-Cerebellum-Cortex\", \"Right-Cerebellum-Cortex\", \"lhCortexVol\", \"rhCortexVol\", \"CortexVol\",\n",
    "      \"Left-Cerebellum-White-Matter\", \"Right-Cerebellum-White-Matter\",\n",
    "      \"CerebralWhiteMatterVol\", \n",
    "      \"3rd-Ventricle\", \"4th-Ventricle\"\n",
    "   ]\n",
    "X, y = get_data(\"volumes.csv\", ROI, \"combine\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/mohanad/Desktop/research/pd-reproducibility/ml/utils.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(column, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2. [Normalize](#normal)\n",
    "\n",
    "In this section, normalization of the data using \"Normalization 1\" and \"Normaliztion 2\" techniques are implemented. \n",
    "\n",
    "Normalization 1:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}{\\sigma \\;of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}$$\n",
    "\n",
    "Normalization 2:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}{\\sigma \\;of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}$$"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def normalize1(data):\n",
    "    normalizedX = []\n",
    "    \n",
    "    for row in X:\n",
    "        normalizedRow = []\n",
    "        for columnIndex, variable in enumerate(row):\n",
    "            mean = np.mean(data[:, columnIndex])\n",
    "            std = np.std(data[:, columnIndex])\n",
    "            normalizedValue = (variable - mean)/std\n",
    "            normalizedRow.append(normalizedValue)        \n",
    "        normalizedX.append(normalizedRow)\n",
    "        \n",
    "    return np.array(normalizedX)\n",
    "            \n",
    "normalize1(X)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ 1.55335454,  0.96866592,  1.40183806, ...,  1.23027958,\n",
       "         1.28178675,  1.51350306],\n",
       "       [ 0.01989099,  0.04470567, -0.07057706, ..., -0.168547  ,\n",
       "        -0.20556361,  0.01279145],\n",
       "       [ 0.78740875,  0.12263003, -0.45080533, ..., -0.22258116,\n",
       "        -0.22465209,  0.14865698],\n",
       "       ...,\n",
       "       [-0.59219754, -0.19455276,  0.04771861, ...,  0.45704757,\n",
       "         0.4538891 , -0.58305278],\n",
       "       [ 0.69191115,  1.18404692, -0.56112723, ...,  0.8528606 ,\n",
       "         0.88509841, -0.11419325],\n",
       "       [ 0.32937396,  0.29808084, -0.01563457, ...,  1.07436628,\n",
       "         0.99221888,  0.91377117]])"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO: Fetch metadata for every patient"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "def normalize2():\n",
    "    print(\"TODO - Unimplemented\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3. [Train and predict models](#predict)\n",
    "\n",
    "In this section, we define four models being logisitc regression, SVM with linear and radial kernel and a random forest. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Imports"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utilities\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def split_data(X, y, training_split):\n",
    "    '''\n",
    "    The following function splits the training and testing data sets\n",
    "    according to a split [0 - 1] passed.\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = training_split, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### SVM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def svm(X, y, kernelType: str, dataSplit: float, normalize):\n",
    "    print(f\"Running SVM with the following parameters: \\nKernel type: {kernelType}\\nNormalization method: {normalize.__name__}\")\n",
    "    X_train, X_test, y_train, y_test = preprocess.split_data(X, y, dataSplit)\n",
    "    X_val, X_test, y_val, y_test = preprocess.split_data(X_test, y_test, 0.5)\n",
    "    \n",
    "    X_grid = np.concatenate((X_train, X_val))\n",
    "    y_grid = np.concatenate((y_train, y_val))\n",
    "    separation_boundary = [-1 for _ in y_train] + [0 for _ in y_val]\n",
    "    ps = PredefinedSplit(separation_boundary)\n",
    "    \n",
    "    param_grid = {\n",
    "        'C': [1.0, 10.0, 100.0, 1000.0],\n",
    "        'gamma': [0.01, 0.10, 1.00, 10.00],\n",
    "        'kernel': [kernelType]\n",
    "    }\n",
    "\n",
    "    print(f\"param_grid: {param_grid}\")\n",
    "\n",
    "    clf = GridSearchCV(SVC(random_state=0), param_grid, cv=ps)\n",
    "\n",
    "    model = clf.fit(normalize(X_grid), y_grid)\n",
    "    train_acc = model.score(normalize(X_train), y_train)\n",
    "    val_acc = model.score(normalize(X_val), y_val)\n",
    "    test_acc = model.score(normalize(X_test), y_test)\n",
    "    print(f'training score: {round(train_acc, 3)}')\n",
    "    print(f'validation score: {round(val_acc, 3)}')\n",
    "    print(f'testing score: {round(test_acc, 3)}')\n",
    "    print(f'Best model params: {model.best_params_}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def logistic_regression(X, y, dataSplit: float, normalize):\n",
    "    print(f\"Running Logistic Regression with the following parameters: \\nNormalization method: {normalize.__name__}\")\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, dataSplit)\n",
    "    X_val, X_test, y_val, y_test = split_data(X_test, y_test, 0.5)\n",
    "    \n",
    "    X_grid = np.concatenate((X_train, X_val))\n",
    "    y_grid = np.concatenate((y_train, y_val))\n",
    "    separation_boundary = [-1 for _ in y_train] + [0 for _ in y_val]\n",
    "    ps = PredefinedSplit(separation_boundary)\n",
    "    \n",
    "    param_grid = {\n",
    "        'penalty': [\"l1\", \"l2\", \"elasticnet\"],\n",
    "        'C': [1.0, 10.0, 100.0, 1000.0]\n",
    "    }\n",
    "    \n",
    "    clf = GridSearchCV(LogisticRegression(random_state=0), param_grid, cv=ps)\n",
    "\n",
    "    model = clf.fit(normalize(X_grid), y_grid)\n",
    "    train_acc = model.score(normalize(X_train), y_train)\n",
    "    val_acc = model.score(normalize(X_val), y_val)\n",
    "    test_acc = model.score(normalize(X_test), y_test)\n",
    "    print(f'training score: {round(train_acc, 3)}')\n",
    "    print(f'validation score: {round(val_acc, 3)}')\n",
    "    print(f'testing score: {round(test_acc, 3)}')\n",
    "    print(f'Best model params: {model.best_params_}')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}