{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cb3f3e9",
   "metadata": {},
   "source": [
    "# Machine Learning pipeline\n",
    "\n",
    "In this notebook, we go through the machine learning pipeline to reproduce Lydia Chougar's paper. The following sections will be covered:\n",
    "\n",
    "[1. Convert CSV to DataFrame](#data)\n",
    "\n",
    "[2. Normalize data](#normalize)\n",
    "\n",
    "[3. Define models](#models)\n",
    "\n",
    "[4. Training models](#training)\n",
    "\n",
    "[5. Results](#results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc488c91",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b42237f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, utils, sys, os, json\n",
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f7ea21",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804b341a",
   "metadata": {},
   "source": [
    "Converts data from CSV to DataFrame and applies any function. \n",
    "- \"combine\": sums all Left and Right regions into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac891096",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(csvFileName: str, ROI: [], heuristic = None):\n",
    "    '''\n",
    "    The following function will sanitize data and build a numpy array with X ROI's volumes and y being the class [NC, PD]\n",
    "    @csvFileName: input volumes csv\n",
    "    @ROI: regions of interests desired\n",
    "    @heuristic: function key\n",
    "    '''\n",
    "    df = pd.read_csv(csvFileName)\n",
    "    df = utils.remove_unwanted_columns(df, ROI)\n",
    "    \n",
    "    if heuristic == \"combine\":\n",
    "        df = utils.combine_left_right_vol(df)\n",
    "        \n",
    "    cols = list(df.columns.values)\n",
    "    cols.pop(cols.index(\"subjectId\"))\n",
    "    df = df[[\"subjectId\"]+cols]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b76c0a",
   "metadata": {},
   "source": [
    "Test *get_data()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba8c71a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subjectId</th>\n",
       "      <th>Pallidum</th>\n",
       "      <th>Putamen</th>\n",
       "      <th>Caudate</th>\n",
       "      <th>Cerebellum-Cortex</th>\n",
       "      <th>Cerebellum-White-Matter</th>\n",
       "      <th>3rd-Ventricle</th>\n",
       "      <th>4th-Ventricle</th>\n",
       "      <th>lhCortexVol</th>\n",
       "      <th>rhCortexVol</th>\n",
       "      <th>CortexVol</th>\n",
       "      <th>CerebralWhiteMatterVol</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3572</td>\n",
       "      <td>3234.8</td>\n",
       "      <td>9403.7</td>\n",
       "      <td>6425.5</td>\n",
       "      <td>101202.1</td>\n",
       "      <td>25991.9</td>\n",
       "      <td>809.7</td>\n",
       "      <td>1595.2</td>\n",
       "      <td>214211.281877</td>\n",
       "      <td>214614.080385</td>\n",
       "      <td>428825.362262</td>\n",
       "      <td>398502.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3756</td>\n",
       "      <td>4021.1</td>\n",
       "      <td>11431.6</td>\n",
       "      <td>8336.5</td>\n",
       "      <td>112507.9</td>\n",
       "      <td>27906.7</td>\n",
       "      <td>1677.0</td>\n",
       "      <td>3021.0</td>\n",
       "      <td>241712.658555</td>\n",
       "      <td>250985.173323</td>\n",
       "      <td>492697.831879</td>\n",
       "      <td>475569.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3768</td>\n",
       "      <td>4240.4</td>\n",
       "      <td>9828.9</td>\n",
       "      <td>7275.3</td>\n",
       "      <td>108180.9</td>\n",
       "      <td>31634.3</td>\n",
       "      <td>2087.1</td>\n",
       "      <td>2487.0</td>\n",
       "      <td>240319.225000</td>\n",
       "      <td>236183.634262</td>\n",
       "      <td>476502.859262</td>\n",
       "      <td>526061.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3369</td>\n",
       "      <td>3871.7</td>\n",
       "      <td>11068.1</td>\n",
       "      <td>7607.9</td>\n",
       "      <td>98919.4</td>\n",
       "      <td>27742.0</td>\n",
       "      <td>918.2</td>\n",
       "      <td>1360.3</td>\n",
       "      <td>227101.242448</td>\n",
       "      <td>229470.755367</td>\n",
       "      <td>456571.997815</td>\n",
       "      <td>420273.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4004</td>\n",
       "      <td>3584.6</td>\n",
       "      <td>9379.7</td>\n",
       "      <td>5883.9</td>\n",
       "      <td>93553.2</td>\n",
       "      <td>25869.0</td>\n",
       "      <td>1374.2</td>\n",
       "      <td>2325.2</td>\n",
       "      <td>215114.677435</td>\n",
       "      <td>213485.100332</td>\n",
       "      <td>428599.777767</td>\n",
       "      <td>388901.0</td>\n",
       "      <td>NC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>3366</td>\n",
       "      <td>5022.9</td>\n",
       "      <td>9982.7</td>\n",
       "      <td>6595.5</td>\n",
       "      <td>105510.8</td>\n",
       "      <td>38319.9</td>\n",
       "      <td>1169.8</td>\n",
       "      <td>1489.1</td>\n",
       "      <td>221069.501788</td>\n",
       "      <td>222936.985582</td>\n",
       "      <td>444006.487370</td>\n",
       "      <td>503456.0</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>3367</td>\n",
       "      <td>4348.7</td>\n",
       "      <td>11044.1</td>\n",
       "      <td>6933.7</td>\n",
       "      <td>105346.1</td>\n",
       "      <td>31321.5</td>\n",
       "      <td>1101.7</td>\n",
       "      <td>1875.8</td>\n",
       "      <td>222235.310341</td>\n",
       "      <td>227057.206724</td>\n",
       "      <td>449292.517066</td>\n",
       "      <td>472333.0</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>3116</td>\n",
       "      <td>4310.3</td>\n",
       "      <td>9121.3</td>\n",
       "      <td>6645.2</td>\n",
       "      <td>101564.1</td>\n",
       "      <td>28589.7</td>\n",
       "      <td>1863.4</td>\n",
       "      <td>2550.6</td>\n",
       "      <td>198129.877854</td>\n",
       "      <td>201670.182642</td>\n",
       "      <td>399800.060496</td>\n",
       "      <td>404765.0</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>3587</td>\n",
       "      <td>4172.0</td>\n",
       "      <td>10429.8</td>\n",
       "      <td>8795.1</td>\n",
       "      <td>123481.9</td>\n",
       "      <td>28049.4</td>\n",
       "      <td>1282.8</td>\n",
       "      <td>2278.8</td>\n",
       "      <td>282485.457627</td>\n",
       "      <td>284700.760325</td>\n",
       "      <td>567186.217952</td>\n",
       "      <td>501236.0</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>3829</td>\n",
       "      <td>4655.8</td>\n",
       "      <td>8916.2</td>\n",
       "      <td>6840.7</td>\n",
       "      <td>102689.1</td>\n",
       "      <td>31238.7</td>\n",
       "      <td>1666.8</td>\n",
       "      <td>1258.1</td>\n",
       "      <td>216814.479354</td>\n",
       "      <td>217215.692236</td>\n",
       "      <td>434030.171590</td>\n",
       "      <td>456889.0</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>215 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     subjectId  Pallidum  Putamen  Caudate  Cerebellum-Cortex  \\\n",
       "0         3572    3234.8   9403.7   6425.5           101202.1   \n",
       "1         3756    4021.1  11431.6   8336.5           112507.9   \n",
       "2         3768    4240.4   9828.9   7275.3           108180.9   \n",
       "3         3369    3871.7  11068.1   7607.9            98919.4   \n",
       "4         4004    3584.6   9379.7   5883.9            93553.2   \n",
       "..         ...       ...      ...      ...                ...   \n",
       "210       3366    5022.9   9982.7   6595.5           105510.8   \n",
       "211       3367    4348.7  11044.1   6933.7           105346.1   \n",
       "212       3116    4310.3   9121.3   6645.2           101564.1   \n",
       "213       3587    4172.0  10429.8   8795.1           123481.9   \n",
       "214       3829    4655.8   8916.2   6840.7           102689.1   \n",
       "\n",
       "     Cerebellum-White-Matter  3rd-Ventricle  4th-Ventricle    lhCortexVol  \\\n",
       "0                    25991.9          809.7         1595.2  214211.281877   \n",
       "1                    27906.7         1677.0         3021.0  241712.658555   \n",
       "2                    31634.3         2087.1         2487.0  240319.225000   \n",
       "3                    27742.0          918.2         1360.3  227101.242448   \n",
       "4                    25869.0         1374.2         2325.2  215114.677435   \n",
       "..                       ...            ...            ...            ...   \n",
       "210                  38319.9         1169.8         1489.1  221069.501788   \n",
       "211                  31321.5         1101.7         1875.8  222235.310341   \n",
       "212                  28589.7         1863.4         2550.6  198129.877854   \n",
       "213                  28049.4         1282.8         2278.8  282485.457627   \n",
       "214                  31238.7         1666.8         1258.1  216814.479354   \n",
       "\n",
       "       rhCortexVol      CortexVol  CerebralWhiteMatterVol class  \n",
       "0    214614.080385  428825.362262                398502.0    NC  \n",
       "1    250985.173323  492697.831879                475569.0    NC  \n",
       "2    236183.634262  476502.859262                526061.0    NC  \n",
       "3    229470.755367  456571.997815                420273.0    NC  \n",
       "4    213485.100332  428599.777767                388901.0    NC  \n",
       "..             ...            ...                     ...   ...  \n",
       "210  222936.985582  444006.487370                503456.0    PD  \n",
       "211  227057.206724  449292.517066                472333.0    PD  \n",
       "212  201670.182642  399800.060496                404765.0    PD  \n",
       "213  284700.760325  567186.217952                501236.0    PD  \n",
       "214  217215.692236  434030.171590                456889.0    PD  \n",
       "\n",
       "[215 rows x 13 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI = [\n",
    "      \"subjectId\", \"class\",\n",
    "      \"Left-Putamen\", \"Right-Putamen\", \n",
    "      \"Right-Caudate\", \"Left-Caudate\", \n",
    "      \"Right-Thalamus-Proper\", \"Left-Thalamus-Proper\", \n",
    "      \"Left-Pallidum\", \"Right-Pallidum\", \n",
    "      \"Left-Cerebellum-Cortex\", \"Right-Cerebellum-Cortex\", \"lhCortexVol\", \"rhCortexVol\", \"CortexVol\",\n",
    "      \"Left-Cerebellum-White-Matter\", \"Right-Cerebellum-White-Matter\",\n",
    "      \"CerebralWhiteMatterVol\", \n",
    "      \"3rd-Ventricle\", \"4th-Ventricle\"\n",
    "   ]\n",
    "df = get_data(\"volumes.csv\", ROI, \"combine\")\n",
    "# df = get_data(\"volumes.csv\", ROI, \"combine\", getDf=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e909a3d1",
   "metadata": {},
   "source": [
    "# Normalize\n",
    "\n",
    "In this section, normalization of the data using \"Normalization 1\" and \"Normaliztion 2\" techniques are implemented. \n",
    "\n",
    "Normalization 1:\n",
    "\n",
    "$$\\dfrac{Variable â€“ mean \\; of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}{\\sigma \\;of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}$$\n",
    "\n",
    "Normalization 2:\n",
    "\n",
    "$$\\dfrac{Variable â€“ mean \\; of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}{\\sigma \\;of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e12cafd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize1(df, mean, std):\n",
    "    if mean is None and std is None:\n",
    "        mean = df.mean(axis=0)\n",
    "        std = df.std(axis=0)\n",
    "        normalizedDf = (df - mean)/std\n",
    "        return normalizedDf.values, mean, std\n",
    "\n",
    "    normalizedDf = (df - mean)/std\n",
    "    return normalizedDf.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16a4221f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.70710678, -0.70710678],\n",
       "       [ 0.70710678,  0.70710678,  0.70710678]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing normalization1\n",
    "trainDf = pd.DataFrame(np.array([[1, 2, 3], [3, 4, 7]]),columns=['a', 'b', 'c'])\n",
    "testDf = pd.DataFrame(np.array([[2, 6, 4], [3, 7, 9]]),columns=['a', 'b', 'c'])\n",
    "normTrainDf, trainMean, trainStd = normalize1(trainDf, None, None)\n",
    "normTestDf = normalize1(testDf, trainMean, trainStd)\n",
    "normTrainDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c572ae77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2(df):\n",
    "    df_no_id = df.drop(columns=[\"subjectId\", \"class\"])\n",
    "    metadata_df = utils.parse_metadata()\n",
    "    merged_df = pd.merge(df, metadata_df, on=[\"subjectId\"])\n",
    "    \n",
    "    stats = {}\n",
    "    for scanner in merged_df[\"scannerType\"].unique():\n",
    "        mean, std = utils.get_mean_and_stats(merged_df.drop(columns=\"subjectId\"), scanner)\n",
    "        stats[scanner] = {\n",
    "            \"mean\": mean.to_dict(),\n",
    "            \"std\": std.to_dict()\n",
    "        }\n",
    "        \n",
    "    for index in merged_df.index:\n",
    "        rowInfo = merged_df.iloc[index]\n",
    "        scanner = rowInfo[\"scannerType\"]\n",
    "        mean = list(stats[scanner][\"mean\"].values())\n",
    "        std = list(stats[scanner][\"std\"].values())\n",
    "        df_no_id.iloc[index] = (df_no_id.iloc[index]-mean)/std\n",
    "        \n",
    "    return df_no_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c20902c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohanad/Desktop/research/pd-reproducibility/ml/utils.py:55: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  mean = queryDf.mean()\n",
      "/home/mohanad/Desktop/research/pd-reproducibility/ml/utils.py:56: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
      "  std = queryDf.std()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678, -0.70710678, -0.70710678, -0.70710678, -0.70710678,\n",
       "        -0.70710678, -0.70710678, -0.70710678, -0.70710678, -0.70710678,\n",
       "        -0.70710678],\n",
       "       [ 0.70710678,  0.70710678,  0.70710678,  0.70710678,  0.70710678,\n",
       "         0.70710678,  0.70710678,  0.70710678,  0.70710678,  0.70710678,\n",
       "         0.70710678],\n",
       "       [ 1.82235096,  2.68279471,  0.65286694,  1.58555251,  6.58397649,\n",
       "         0.69631596,  2.55149474, 11.59372081,  2.76593576,  4.94406413,\n",
       "         2.81328748],\n",
       "       [ 0.70710678,  0.70710678,  0.70710678, -0.70710678,  0.70710678,\n",
       "        -0.70710678, -0.70710678, -0.70710678, -0.70710678, -0.70710678,\n",
       "         0.70710678],\n",
       "       [-0.70710678, -0.70710678, -0.70710678,  0.70710678, -0.70710678,\n",
       "         0.70710678,  0.70710678,  0.70710678,  0.70710678,  0.70710678,\n",
       "        -0.70710678]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing normalization1\n",
    "df_norm2 = get_data(\"test.csv\", ROI, \"combine\")\n",
    "normalize2Df = normalize2(df_norm2)\n",
    "normalize2Df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52157a2b",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "In this section, we define four models being logisitc regression, SVM with linear and radial kernel and a random forest. As per the paper:\n",
    "\n",
    "_Using the scikit-learn package, four supervised\n",
    "machine learning algorithms were used: logistic regression, support vector machine (SVM) with a linear kernel, SVM with a radial basis function kernel, and\n",
    "random forest_ (Chougar et al.)\n",
    "\n",
    "Additionally, we will implement a stratified cross validation loop for hyperparameter tuning. As per the paper:\n",
    "\n",
    "_The cross-validation procedure on the training cohort included two nested loops: an outer loop with repeated stratified random splits with 50 repetitions evaluating the classification performances and an inner loop with 5 fold cross-validation used to optimize the hyperparameters of the algorithms_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25f2dd",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9bd77e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "# Parallel job\n",
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca175b76",
   "metadata": {},
   "source": [
    "### Generic definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f27ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(df, modelType, reportKey, normalize, paramGrid, dataFile, ROI, heuristic=None):   \n",
    "    print(f\"\\n======================Running {modelType} with the following parameters======================\\nNormalization: {normalize.__name__}\\nParam Grid: {paramGrid}\\nData: {dataFile}\\nROI: {ROI}\")\n",
    "    performanceDict = {}\n",
    "    X = df.values\n",
    "    y = utils.convert_Y(X[:, -1])\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=50, random_state=42)\n",
    "    \n",
    "    # Define model type\n",
    "    if modelType == \"SVM\":\n",
    "        clf = GridSearchCV(SVC(random_state=0), paramGrid)\n",
    "    elif modelType == \"RF\":\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=0), paramGrid)\n",
    "    elif modelType == \"LR\":\n",
    "        clf = GridSearchCV(LogisticRegression(random_state=0), paramGrid)\n",
    "    \n",
    "    iteration = 0\n",
    "    for train_index, test_index in cv.split(X, y):        \n",
    "        print(f\"=================Iteration #{iteration}=================\")\n",
    "        \n",
    "        # Get fold data train/test sets\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        print(f'Shape of train set: {X_train.shape}')\n",
    "        print(f'Shape of test set: {X_test.shape}')\n",
    "        \n",
    "        # Normalize model data\n",
    "        print(\"Normalizing data...\")\n",
    "        if normalize.__name__ == \"normalize1\":\n",
    "            trainDf = pd.DataFrame(X_train, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "            testDf = pd.DataFrame(X_test, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "            X_train_normalized, mean_train, std_train = normalize(trainDf, None, None)\n",
    "            X_test_normalized = normalize(testDf, mean_train, std_train)\n",
    "\n",
    "        elif normalize.__name__ == \"normalize2\":\n",
    "            trainDf = pd.DataFrame(X_train, columns=columns)\n",
    "            testDf = pd.DataFrame(X_test, columns=columns)\n",
    "            X_train_normalized = normalize2(trainDf)\n",
    "            X_test_normalized = normalize2(testDf)\n",
    "            \n",
    "        print(\"Done normalizing data\")\n",
    "            \n",
    "        print(\"Fitting model...\")\n",
    "        model = clf.fit(X_train_normalized, y_train)\n",
    "        print(\"Done fitting model\")\n",
    "        \n",
    "        print(\"Computing results metrics...\")\n",
    "        utils.performance_report(performanceDict, model, modelType, reportKey, iteration, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "        print(\"Done computing results metrics\\n\")\n",
    "        iteration+=1\n",
    "        \n",
    "    with open(f\"report.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(performanceDict, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return performanceDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39556755",
   "metadata": {},
   "source": [
    "### Parallel model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb35e042",
   "metadata": {},
   "source": [
    "Since our cross validation loop produces 250 folds per model, it is bound to take a long time to run. Therefore, a refined version of the code above is re-written in parallel. \n",
    "\n",
    "It is recommended that you run the following code from your terminal:\n",
    "```\n",
    "conda activate research # Check README to get corect CONDA environemnt\n",
    "cd ml/\n",
    "python run.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b26683c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(clf, train_index, test_index, X, y, normalize, columns, modelType, reportKey, iteration):\n",
    "    print(f\"=================Iteration #{iteration}=================\")\n",
    "    performanceDict = {}\n",
    "        \n",
    "    # Get fold data train/test sets\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(f'Shape of train set: {X_train.shape}')\n",
    "    print(f'Shape of test set: {X_test.shape}')\n",
    "    \n",
    "    # Normalize model data\n",
    "    print(\"Normalizing data...\")\n",
    "    if normalize.__name__ == \"normalize1\":\n",
    "        trainDf = pd.DataFrame(X_train, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "        testDf = pd.DataFrame(X_test, columns=columns).drop(columns=[\"subjectId\", \"class\"])\n",
    "        X_train_normalized, mean_train, std_train = normalize(trainDf, None, None)\n",
    "        X_test_normalized = normalize(testDf, mean_train, std_train)\n",
    "\n",
    "    elif normalize.__name__ == \"normalize2\":\n",
    "        trainDf = pd.DataFrame(X_train, columns=columns)\n",
    "        testDf = pd.DataFrame(X_test, columns=columns)\n",
    "        X_train_normalized = normalize2(trainDf)\n",
    "        X_test_normalized = normalize2(testDf)\n",
    "        \n",
    "    print(\"Done normalizing data\")\n",
    "        \n",
    "    print(f\"Fitting {modelType} model #{iteration}...\")\n",
    "    model = clf.fit(X_train_normalized, y_train)\n",
    "    print(\"Done fitting model\")\n",
    "    \n",
    "    print(f\"Computing results metrics for {modelType} model #{iteration}...\")\n",
    "    performanceDict = utils.performance_report(model, modelType, reportKey, iteration, X_train_normalized, X_test_normalized, y_train, y_test)\n",
    "    print(\"Done computing results metrics\\n\")\n",
    "\n",
    "    return performanceDict\n",
    "\n",
    "def parallel_model(df, modelType, reportKey, normalize, paramGrid, dataFile, ROI, heuristic=None):\n",
    "    print(f\"\\n======================Running {modelType} with the following parameters======================\\nNormalization: {normalize.__name__}\\nParam Grid: {paramGrid}\\nData: {dataFile}\\nROI: {ROI}\")\n",
    "\n",
    "    performance = []\n",
    "    if not os.path.isdir(modelType):\n",
    "        os.mkdir(modelType)\n",
    "\n",
    "    X = df.values\n",
    "    y = utils.convert_Y(X[:, -1])\n",
    "    columns = df.columns\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=2, n_repeats=3, random_state=42)\n",
    "\n",
    "    # Define model type\n",
    "    if modelType == \"SVM\":\n",
    "        clf = GridSearchCV(SVC(random_state=0), paramGrid)\n",
    "    elif modelType == \"RF\":\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=0, n_jobs = -1), paramGrid)\n",
    "    elif modelType == \"LR\":\n",
    "        clf = GridSearchCV(LogisticRegression(random_state=0), paramGrid)\n",
    "    \n",
    "    output = Parallel(n_jobs=-1)(delayed(train)(clf, train_index, test_index, X, y, normalize, columns, modelType, reportKey, iteration) for iteration, (train_index, test_index) in enumerate(cv.split(X, y)))\n",
    "\n",
    "    performance.append(output)\n",
    "\n",
    "    with open(f\"{modelType}/{reportKey}_report.json\", 'w', encoding='utf-8') as f:\n",
    "        json.dump(performance, f, ensure_ascii=False, indent=4)\n",
    "        \n",
    "    return performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf82118b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f9c550",
   "metadata": {},
   "source": [
    "### Define parameter grid per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "965ffebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_LINEAR_PARAMS = {\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0],\n",
    "    'gamma': [0.01, 0.10, 1.00, 10.00],\n",
    "    'kernel': ['linear']\n",
    "}\n",
    "\n",
    "SVM_RBF_PARAMS = {\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0],\n",
    "    'gamma': [0.01, 0.10, 1.00, 10.00],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "LR_PARAMS = {\n",
    "    'penalty': [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "\n",
    "RF_PARAMS = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 4, 5, 10, 13],\n",
    "    'min_samples_leaf': [1, 2, 5, 8, 13]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e8984",
   "metadata": {},
   "source": [
    "### Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5049ea1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data(\"volumes.csv\", ROI, \"combine\")\n",
    "\n",
    "# SVM - Linear\n",
    "# model(df, \"SVM\", \"linear_svm_norm1\", normalize1, SVM_LINEAR_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "# parallel_model(df, \"SVM\", \"linear_svm_norm1\", normalize2, SVM_LINEAR_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "\n",
    "# # SVM - RBF\n",
    "# model(X, y, \"SVM\", \"rbf_svm_norm1\", 0.7, normalize1, SVM_RBF_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "# model(X, y, \"SVM\", \"rbf_svm_norm1\", 0.7, normalize2, SVM_RBF_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "\n",
    "# # Logistic Regression\n",
    "# model(X, y, \"LR\", \"lr_norm1\", 0.7, normalize1, LR_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "# model(X, y, \"LR\", \"lr_norm2\", 0.7, normalize2, LR_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "\n",
    "# # Random Forest\n",
    "# model(X, y, \"RF\", \"rf_norm1\", 0.7, normalize1, RF_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")\n",
    "# model(X, y, \"RF\", \"rf_norm2\",0.7, normalize2, RF_PARAMS, \"volumes.csv\", ROI, heuristic=\"combine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac2770e",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd1534",
   "metadata": {},
   "source": [
    "Results can be found in the [analysis notebook](analysis.ipynb) for a detailed breakdown of the models and comparasions to the papers' results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
