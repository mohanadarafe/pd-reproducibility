{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning pipeline\n",
    "\n",
    "In this notebook, we go through the machine learning pipeline to reproduce Lydia Chougar's paper. The following sections will be covered:\n",
    "\n",
    "1 - Convert CSV to DataFrame\n",
    "\n",
    "2 - Normalize\n",
    "\n",
    "3 - Train and predict models\n",
    "\n",
    "4 - Cross Validation\n",
    "\n",
    "5 - Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, utils, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert CSV to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converts data from CSV to DataFrame and applies any function. \n",
    "- \"combine\": sums all Left and Right regions into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(csvFileName: str, ROI: [], heuristic = None):\n",
    "    '''\n",
    "    The following function will sanitize data and build a numpy array with X ROI's volumes and y being the class [NC, PD]\n",
    "    @csvFileName: input volumes csv\n",
    "    @ROI: regions of interests desired\n",
    "    @heuristic: function key\n",
    "    '''\n",
    "    df = pd.read_csv(csvFileName)\n",
    "    df = utils.remove_unwanted_columns(df, ROI)\n",
    "    \n",
    "    if heuristic == \"combine\":\n",
    "        df = utils.combine_left_right_vol(df)\n",
    "        \n",
    "    arr = df.values\n",
    "    X = arr[:, :-1]\n",
    "    y = utils.convert_Y(arr[:, -1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test *get_data()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/glatard/code/pd-reproducibility/ml/utils.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(column, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4805.9, 10689.0, 8204.3, ..., 260137.320394, 521837.72897,\n",
       "        541416.0],\n",
       "       [4025.5, 9543.6, 6856.3, ..., 227892.928374, 453029.527784,\n",
       "        459966.0],\n",
       "       [4416.1, 9640.2, 6508.2, ..., 226647.385438, 452146.451741,\n",
       "        467340.0],\n",
       "       ...,\n",
       "       [3714.0, 9247.0, 6964.6, ..., 242313.526846, 483537.306049,\n",
       "        427627.0],\n",
       "       [4367.5, 10956.0, 6407.2, ..., 251437.424595, 503486.026837,\n",
       "        453074.0],\n",
       "       [4183.0, 9857.7, 6906.6, ..., 256543.358332, 508441.662564,\n",
       "        508866.0]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI = [\n",
    "      \"class\",\n",
    "      \"Left-Putamen\", \"Right-Putamen\", \n",
    "      \"Right-Caudate\", \"Left-Caudate\", \n",
    "      \"Right-Thalamus-Proper\", \"Left-Thalamus-Proper\", \n",
    "      \"Left-Pallidum\", \"Right-Pallidum\", \n",
    "      \"Left-Cerebellum-Cortex\", \"Right-Cerebellum-Cortex\", \"lhCortexVol\", \"rhCortexVol\", \"CortexVol\",\n",
    "      \"Left-Cerebellum-White-Matter\", \"Right-Cerebellum-White-Matter\",\n",
    "      \"CerebralWhiteMatterVol\", \n",
    "      \"3rd-Ventricle\", \"4th-Ventricle\"\n",
    "   ]\n",
    "X, y = get_data(\"volumes.csv\", ROI, \"combine\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. [Normalize](#normal)\n",
    "\n",
    "In this section, normalization of the data using \"Normalization 1\" and \"Normaliztion 2\" techniques are implemented. \n",
    "\n",
    "Normalization 1:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}{\\sigma \\;of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}$$\n",
    "\n",
    "Normalization 2:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}{\\sigma \\;of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: remove loop\n",
    "\n",
    "def normalize1(data, mean, std):\n",
    "    df = pd.DataFrame(data=data)\n",
    "\n",
    "    if mean is None and std is None:\n",
    "        mean = df.mean(axis=0)\n",
    "        std = df.std(axis=0)\n",
    "        print(mean)\n",
    "        print(std)\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        df[i] = df[i].apply(lambda x: (x-mean[i])/std[i])\n",
    "\n",
    "    return df.values, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c\n",
      "0  1  2  3\n",
      "1  3  4  7\n",
      "0    2.0\n",
      "1    3.0\n",
      "2    5.0\n",
      "dtype: float64\n",
      "0    1.414214\n",
      "1    1.414214\n",
      "2    2.828427\n",
      "dtype: float64\n",
      "[[-0.70710678 -0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Testing normalization1\n",
    "testDf = pd.DataFrame(np.array([[1, 2, 3], [3, 4, 7]]),columns=['a', 'b', 'c'])\n",
    "print(testDf)\n",
    "normalizedDfTest, mean, std = normalize1(testDf.values, None, None)\n",
    "print(normalizedDfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Fetch metadata for every patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2():\n",
    "    print(\"TODO - Unimplemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. [Train and predict models](#predict)\n",
    "\n",
    "In this section, we define four models being logisitc regression, SVM with linear and radial kernel and a random forest. As per the paper:\n",
    "\n",
    "_Using the scikit-learn package, four supervised\n",
    "machine learning algorithms were used: logistic regression, support vector machine (SVM) with a linear kernel, SVM with a radial basis function kernel, and\n",
    "random forest_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, training_split):\n",
    "    '''\n",
    "    The following function splits the training and testing data sets\n",
    "    according to a split [0 - 1] passed.\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = training_split, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_model_score(model, X_train, y_train, X_test, y_test):\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f'training score: {round(train_acc, 3)}')\n",
    "    print(f'testing score: {round(test_acc, 3)}')\n",
    "    return train_acc, test_acc\n",
    "\n",
    "def model(X, y, modelType, dataSplit, normalize, paramGrid):\n",
    "    # Define training, validation and test sets\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, dataSplit)\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=50)\n",
    "    \n",
    "    # Define model type\n",
    "    if modelType == \"SVM\":\n",
    "        clf = GridSearchCV(SVC(random_state=0), paramGrid, cv=cv)\n",
    "    elif modelType == \"RF\":\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=0), paramGrid, cv=cv)\n",
    "    elif modelType == \"LR\":\n",
    "        clf = GridSearchCV(LogisticRegression(random_state=0), paramGrid, cv=cv)\n",
    "        \n",
    "    # Normalize model data\n",
    "    if normalize.__name__ == \"normalize1\":\n",
    "        X_grid_normalized, mean_train, std_train = normalize(X_grid, None, None)\n",
    "        X_test_normalized, mean_test, std_test = normalize(X_test, mean_train, std_train)\n",
    "        \n",
    "    # Fit and predict\n",
    "    model = clf.fit(X_grid_normalized, y_grid)\n",
    "    train_acc, test_acc = get_model_score(X_grid_normalized, y_grid, X_test_normalized, y_test)\n",
    "    print(f'Best model params: {model.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Parameter grid for parameter (kernel) needs to be a list or numpy array, but got (<class 'str'>). Single values need to be wrapped in a list with one element.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e2718597bc1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkernelType\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rbf\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mparam_grid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"kernel\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkernelType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-55b924121397>\u001b[0m in \u001b[0;36mmodel\u001b[0;34m(X, y, modelType, dataSplit, normalize, paramGrid)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Define model type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmodelType\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSVC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmodelType\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"RF\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparamGrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, estimator, param_grid, scoring, n_jobs, refit, cv, verbose, pre_dispatch, error_score, return_train_score)\u001b[0m\n\u001b[1;32m   1290\u001b[0m             return_train_score=return_train_score)\n\u001b[1;32m   1291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         \u001b[0m_check_param_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_check_param_grid\u001b[0;34m(param_grid)\u001b[0m\n\u001b[1;32m    398\u001b[0m             if (isinstance(v, str) or\n\u001b[1;32m    399\u001b[0m                     not isinstance(v, (np.ndarray, Sequence))):\n\u001b[0;32m--> 400\u001b[0;31m                 raise ValueError(\"Parameter grid for parameter ({0}) needs to\"\n\u001b[0m\u001b[1;32m    401\u001b[0m                                  \u001b[0;34m\" be a list or numpy array, but got ({1}).\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m                                  \u001b[0;34m\" Single values need to be wrapped in a list\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Parameter grid for parameter (kernel) needs to be a list or numpy array, but got (<class 'str'>). Single values need to be wrapped in a list with one element."
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0],\n",
    "    'gamma': [0.01, 0.10, 1.00, 10.00]\n",
    "}\n",
    "for kernelType in [\"linear\", \"rbf\"]:\n",
    "    param_grid[\"kernel\"] = kernelType\n",
    "    model(X, y, \"SVM\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'penalty': [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "model(X, y, \"LR\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 4, 5, 10, 13],\n",
    "    'min_samples_leaf': [1, 2, 5, 8, 13]\n",
    "}\n",
    "model(X, y, \"RF\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. [Cross Validation](#crossval)\n",
    "\n",
    "In this section, we will implement the cross validation loop used in the paper. As per the paper:\n",
    "\n",
    "_The cross-validation procedure on the training cohort included two nested loops: an outer loop with repeated stratified random splits with 50 repetitions evaluating the classification performances and an inner loop with 5 fold cross-validation used to optimize the hyperparameters of the algorithms_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
