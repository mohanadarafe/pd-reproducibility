{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99d6649d",
   "metadata": {},
   "source": [
    "# Machine Learning pipeline\n",
    "\n",
    "In this notebook, we go through the machine learning pipeline to reproduce Lydia Chougar's paper. The following sections will be covered:\n",
    "\n",
    "1 - Convert CSV to DataFrame\n",
    "\n",
    "2 - Normalize\n",
    "\n",
    "3 - Train and predict models\n",
    "\n",
    "4 - Cross Validation\n",
    "\n",
    "5 - Results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8372b24",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "287343e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob, utils, sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d77286",
   "metadata": {},
   "source": [
    "# Convert CSV to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56044551",
   "metadata": {},
   "source": [
    "Converts data from CSV to DataFrame and applies any function. \n",
    "- \"combine\": sums all Left and Right regions into one column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fb170b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(csvFileName: str, ROI: [], heuristic = None):\n",
    "    '''\n",
    "    The following function will sanitize data and build a numpy array with X ROI's volumes and y being the class [NC, PD]\n",
    "    @csvFileName: input volumes csv\n",
    "    @ROI: regions of interests desired\n",
    "    @heuristic: function key\n",
    "    '''\n",
    "    df = pd.read_csv(csvFileName)\n",
    "    df = utils.remove_unwanted_columns(df, ROI)\n",
    "    \n",
    "    if heuristic == \"combine\":\n",
    "        df = utils.combine_left_right_vol(df)\n",
    "        \n",
    "    arr = df.values\n",
    "    X = arr[:, :-1]\n",
    "    y = utils.convert_Y(arr[:, -1])\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1588185c",
   "metadata": {},
   "source": [
    "Test *get_data()* function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "68c39a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mohanad/Desktop/research/pd-reproducibility/ml/utils.py:10: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n",
      "  df = df.drop(column, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4805.9, 10689.0, 8204.3, ..., 260137.320394, 521837.72897,\n",
       "        541416.0],\n",
       "       [4025.5, 9543.6, 6856.3, ..., 227892.928374, 453029.527784,\n",
       "        459966.0],\n",
       "       [4416.1, 9640.2, 6508.2, ..., 226647.385438, 452146.451741,\n",
       "        467340.0],\n",
       "       ...,\n",
       "       [3714.0, 9247.0, 6964.6, ..., 242313.526846, 483537.306049,\n",
       "        427627.0],\n",
       "       [4367.5, 10956.0, 6407.2, ..., 251437.424595, 503486.026837,\n",
       "        453074.0],\n",
       "       [4183.0, 9857.7, 6906.6, ..., 256543.358332, 508441.662564,\n",
       "        508866.0]], dtype=object)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI = [\n",
    "      \"class\",\n",
    "      \"Left-Putamen\", \"Right-Putamen\", \n",
    "      \"Right-Caudate\", \"Left-Caudate\", \n",
    "      \"Right-Thalamus-Proper\", \"Left-Thalamus-Proper\", \n",
    "      \"Left-Pallidum\", \"Right-Pallidum\", \n",
    "      \"Left-Cerebellum-Cortex\", \"Right-Cerebellum-Cortex\", \"lhCortexVol\", \"rhCortexVol\", \"CortexVol\",\n",
    "      \"Left-Cerebellum-White-Matter\", \"Right-Cerebellum-White-Matter\",\n",
    "      \"CerebralWhiteMatterVol\", \n",
    "      \"3rd-Ventricle\", \"4th-Ventricle\"\n",
    "   ]\n",
    "X, y = get_data(\"volumes.csv\", ROI, \"combine\")\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c96f6",
   "metadata": {},
   "source": [
    "# 2. [Normalize](#normal)\n",
    "\n",
    "In this section, normalization of the data using \"Normalization 1\" and \"Normaliztion 2\" techniques are implemented. \n",
    "\n",
    "Normalization 1:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}{\\sigma \\;of \\;PD \\;and \\;NC \\;in \\;the \\;training \\;cohort}$$\n",
    "\n",
    "Normalization 2:\n",
    "\n",
    "$$\\dfrac{Variable – mean \\; of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}{\\sigma \\;of \\;controls \\;scanned \\;using \\;the \\;same \\;scanner}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "916fa487",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## TODO: remove loop\n",
    "\n",
    "def normalize1(data, mean, std):\n",
    "    df = pd.DataFrame(data=data)\n",
    "\n",
    "    if mean is None and std is None:\n",
    "        mean = df.mean(axis=0)\n",
    "        std = df.std(axis=0)\n",
    "        print(mean)\n",
    "        print(std)\n",
    "\n",
    "    for i in range(df.shape[1]):\n",
    "        df[i] = df[i].apply(lambda x: (x-mean[i])/std[i])\n",
    "\n",
    "    return df.values, mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "412766c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c\n",
      "0  1  2  3\n",
      "1  3  4  7\n",
      "0    2.0\n",
      "1    3.0\n",
      "2    5.0\n",
      "dtype: float64\n",
      "0    1.414214\n",
      "1    1.414214\n",
      "2    2.828427\n",
      "dtype: float64\n",
      "[[-0.70710678 -0.70710678 -0.70710678]\n",
      " [ 0.70710678  0.70710678  0.70710678]]\n"
     ]
    }
   ],
   "source": [
    "# Testing normalization1\n",
    "testDf = pd.DataFrame(np.array([[1, 2, 3], [3, 4, 7]]),columns=['a', 'b', 'c'])\n",
    "print(testDf)\n",
    "normalizedDfTest, mean, std = normalize1(testDf.values, None, None)\n",
    "print(normalizedDfTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05926a0f",
   "metadata": {},
   "source": [
    "### TODO: Fetch metadata for every patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "640ea10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize2():\n",
    "    print(\"TODO - Unimplemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "772e9488",
   "metadata": {},
   "source": [
    "# 3. [Train and predict models](#predict)\n",
    "\n",
    "In this section, we define four models being logisitc regression, SVM with linear and radial kernel and a random forest. As per the paper:\n",
    "\n",
    "_Using the scikit-learn package, four supervised\n",
    "machine learning algorithms were used: logistic regression, support vector machine (SVM) with a linear kernel, SVM with a radial basis function kernel, and\n",
    "random forest_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da7ad0b",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "4410ed3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Utils\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import PredefinedSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2d2b40",
   "metadata": {},
   "source": [
    "## Utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3bad7bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, training_split):\n",
    "    '''\n",
    "    The following function splits the training and testing data sets\n",
    "    according to a split [0 - 1] passed.\n",
    "    '''\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = training_split, random_state = 42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def get_model_score(model, X_train, y_train, X_test, y_test):\n",
    "    train_acc = model.score(X_train, y_train)\n",
    "    test_acc = model.score(X_test, y_test)\n",
    "    print(f'training score: {round(train_acc, 3)}')\n",
    "    print(f'testing score: {round(test_acc, 3)}')\n",
    "    return train_acc, test_acc\n",
    "\n",
    "def model(X, y, modelType, dataSplit, normalize, paramGrid):\n",
    "    # Define training, validation and test sets\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y, dataSplit)\n",
    "    \n",
    "    # Setup CV\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=50)\n",
    "    \n",
    "    # Define model type\n",
    "    if modelType == \"SVM\":\n",
    "        clf = GridSearchCV(SVC(random_state=0), paramGrid, cv=cv)\n",
    "    elif modelType == \"RF\":\n",
    "        clf = GridSearchCV(RandomForestClassifier(random_state=0), paramGrid, cv=cv)\n",
    "    elif modelType == \"LR\":\n",
    "        clf = GridSearchCV(LogisticRegression(random_state=0), paramGrid, cv=cv)\n",
    "        \n",
    "    # Normalize model data\n",
    "    if normalize.__name__ == \"normalize1\":\n",
    "        X_grid_normalized, mean_train, std_train = normalize(X_grid, None, None)\n",
    "        X_test_normalized, mean_test, std_test = normalize(X_test, mean_train, std_train)\n",
    "        \n",
    "    # Fit and predict\n",
    "    model = clf.fit(X_grid_normalized, y_grid)\n",
    "    train_acc, test_acc = get_model_score(X_grid_normalized, y_grid, X_test_normalized, y_test)\n",
    "    print(f'Best model params: {model.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbce244c",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1a178d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [1.0, 10.0, 100.0, 1000.0], 'gamma': [0.01, 0.1, 1.0, 10.0], 'kernel': 'linear'}\n",
      "{'C': [1.0, 10.0, 100.0, 1000.0], 'gamma': [0.01, 0.1, 1.0, 10.0], 'kernel': 'rbf'}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0],\n",
    "    'gamma': [0.01, 0.10, 1.00, 10.00]\n",
    "}\n",
    "for kernelType in [\"linear\", \"rbf\"]:\n",
    "    param_grid[\"kernel\"] = kernelType\n",
    "    model(X, y, \"SVM\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5a652e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f3c9409",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'penalty': [\"l1\", \"l2\", \"elasticnet\"],\n",
    "    'C': [1.0, 10.0, 100.0, 1000.0]\n",
    "}\n",
    "model(X, y, \"LR\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8fffc9",
   "metadata": {},
   "source": [
    "### Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "331303a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'min_samples_split': [2, 4, 5, 10, 13],\n",
    "    'min_samples_leaf': [1, 2, 5, 8, 13]\n",
    "}\n",
    "model(X, y, \"RF\", 0.7, normalize1, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea86002",
   "metadata": {},
   "source": [
    "# 4. [Cross Validation](#crossval)\n",
    "\n",
    "In this section, we will implement the cross validation loop used in the paper. As per the paper:\n",
    "\n",
    "_The cross-validation procedure on the training cohort included two nested loops: an outer loop with repeated stratified random splits with 50 repetitions evaluating the classification performances and an inner loop with 5 fold cross-validation used to optimize the hyperparameters of the algorithms_ (Chougar et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a16914",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "961d94fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "89343677",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
